### 项目使用说明

#### 一、使用局域网搭建的ollama，并使用ollama平台上的模型

##### 1.1.ubuntu上搭建ollama，并在ollama上安装deepseek-r1:32b
    
- 如果Ollama支持直接安装，你可以通过其官网或文档获取安装包并安装。假设Ollama是通过pip进行部署，你可以在终端中使用如下命令：

> aufe@aufe-To-be-filled-by-O-E-M:~$ curl -fsSL https://ollama.com/install.sh | sh

- 返回结果：
> The Ollama API is now available at 127.0.0.1:11434.
> Install complete. Run "ollama" from the command line.
> AMD GPU ready.

- 设置ollama的默认端口
> echo export OLLAMA_HOST="0.0.0.0:11434">>~/.bashrc

- 查看设置情况
> cat ~/.bashrc

- 激活配置
> source ~/.bashrc

- 启动ollama
> ollama server

- 拉取模型文件
> ollama rum llama3:8b
> ollama run deepseek-r1:32b

- 配置OLLAMA API使其可以局域网内访问
> 1.添加环境参数(在Service下面)：
    sudo vim /etc/systemd/system/ollama.service
    Environment="OLLAMA_HOST=0.0.0.0:11434"

>2.在防火墙中加入端口11434
    sudo ufw allow 11434/tcp

>3.重启系统

- 测试ollama API的指令：

>curl -X POST "http://localhost:11434/api/generate" \
     -H "Content-Type: application/json" \
     -d '{"model":"deepseek-r1:32b", "prompt":"Hello, how are you?"}'
    
>curl "http://192.168.31.172:11434/api/generate" \
	-d '{"model":"deepseek-r1:32b", "prompt":"Hello, how are you?"}'
  
>curl "http://192.168.31.172:11434/api/generate" \
	-d '{\"model\":\"deepseek-r1:32b\", \"prompt\":\"Hello, how are you?\"}'
