[
    {
        "Name": "adaptive_block_size",
        "Title": "Adaptive Block Size: Dynamic Context Window Adjustment for Efficient Training",
        "Experiment": "Modify the model to dynamically adjust its block size during training, starting with a smaller block size and gradually increasing it. This could potentially lead to faster initial training and better long-range dependency learning.",
        "Interestingness": 6,
        "Feasibility": 4,
        "Novelty": 4,
        "novel": false
    },
    {
        "Name": "layerwise_learning_rates",
        "Title": "Layer-wise Learning Rate Adaptation: Optimizing Training Dynamics in Transformer Models",
        "Experiment": "Implement layer-wise learning rates, where each transformer layer has its own learning rate. Modify the configure_optimizers function to assign different learning rates to different layers, with deeper layers having lower learning rates. Compare the training dynamics, convergence speed, and final performance with the baseline model.",
        "Interestingness": 4,
        "Feasibility": 6,
        "Novelty": 2,
        "novel": false
    },
    {
        "Name": "lsh_sparse_attention",
        "Title": "Locality-Sensitive Hashing Attention: Efficient Sparse Interaction Learning in Transformers",
        "Experiment": "Replace standard scaled dot-product attention with LSH-based sparse attention in the transformer layer. Implement hashing of query/key vectors to form buckets of candidate tokens for each token's attention computation. Modify the `attention` function within the `MultiHeadAttention` block to use LSH grouping, parameterized by bucket size and number of hash rounds. Measure training speed (time per batch), final loss, and inference performance on validation data compared to baseline models. Vary hyperparameters like bucket size and hash iterations to identify optimal configurations.",
        "Interestingness": 7,
        "Feasibility": 6,
        "Novelty": 5,
        "novel": false
    }
]